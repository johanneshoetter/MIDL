{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.resnet import ResNet18\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from utils.sorting_utils import sort_one_class, sort_all_classes, weighted_highest_sampling\n",
    "\n",
    "class DataLoader():\n",
    "    \n",
    "    def __init__(self, root='./data', batch_size=64):\n",
    "        self.root = root        \n",
    "        self.trainset = None\n",
    "        self.testset = None\n",
    "        self.known_strategies = ['freeze', 'shuffle', 'homogeneous', 'heterogeneous', 'max_k_loss', 'min_k_loss']\n",
    "        self.seed_incrementer = {strategy: 0 for strategy in self.known_strategies}\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def download_cifar(self):\n",
    "        #print('==> Preparing data..')\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "        # data needs to be loaded through dataloader to get into the correct format\n",
    "        trainset = torchvision.datasets.CIFAR10(root=self.root, train=True, download=True, transform=transform_train)\n",
    "        trainloader = torch.utils.data.DataLoader(trainset, batch_size=self.batch_size, shuffle=False, num_workers=2)\n",
    "        self.X_train, self.Y_train = [], []\n",
    "        for x, y in trainloader:\n",
    "            self.X_train.extend(x.numpy()) #numpy needed for a casting workaround\n",
    "            self.Y_train.extend(y.numpy())\n",
    "        self.X_train = np.array(self.X_train)\n",
    "        self.Y_train = np.array(self.Y_train)\n",
    "        \n",
    "        testset = torchvision.datasets.CIFAR10(root=self.root, train=False, download=True, transform=transform_test)\n",
    "        testloader = torch.utils.data.DataLoader(testset, batch_size=self.batch_size, shuffle=False, num_workers=2)\n",
    "        self.X_test, self.Y_test = [], []\n",
    "        for x, y in testloader:\n",
    "            self.X_test.extend(x.numpy())\n",
    "            self.Y_test.extend(y.numpy())\n",
    "        self.X_test = np.array(self.X_test)\n",
    "        self.Y_test = np.array(self.Y_test)\n",
    "                \n",
    "        self.X_batches_train, self.Y_batches_train = None, None\n",
    "        self.X_batches_test, self.Y_batches_test = None, None\n",
    "            \n",
    "    def yield_batches(self, strategy, use_train=True, random_state=None, criterion=None, device=None, num_iterations=None):\n",
    "        assert strategy in self.known_strategies, 'Unknown action'\n",
    "        \n",
    "        # seeds will be incremented at each epoch, to give the shuffling methods a new random seed\n",
    "        # this will still be deterministic!\n",
    "        current_seed = random_state + self.seed_incrementer[strategy]\n",
    "        self.seed_incrementer[strategy] += 1\n",
    "        \n",
    "        # prepare the data\n",
    "        if strategy == 'freeze':\n",
    "            self.X_batches_train, self.Y_batches_train = self.X_train, self.Y_train\n",
    "            self.X_batches_test, self.Y_batches_test = self.X_test, self.Y_test\n",
    "            yield_batchwise = True\n",
    "        elif strategy == 'shuffle':\n",
    "            self.X_batches_train, self.Y_batches_train = shuffle(self.X_train, self.Y_train, random_state=current_seed)\n",
    "            self.X_batches_test, self.Y_batches_test = shuffle(self.X_test, self.Y_test, random_state=current_seed)\n",
    "            yield_batchwise = True\n",
    "        elif strategy == 'homogeneous':\n",
    "            self.X_batches_train, self.Y_batches_train = sort_one_class(self.X_train, self.Y_train, self.batch_size, \\\n",
    "                                                            use_shuffle=True, random_state=current_seed)\n",
    "            self.X_batches_test, self.Y_batches_test = sort_one_class(self.X_test, self.Y_test, self.batch_size, \\\n",
    "                                                            use_shuffle=True, random_state=current_seed)\n",
    "            yield_batchwise = True\n",
    "        elif strategy == 'heterogeneous':\n",
    "            self.X_batches_train, self.Y_batches_train = sort_all_classes(self.X_train, self.Y_train, self.batch_size, \\\n",
    "                                                              use_shuffle=True, random_state=current_seed)\n",
    "            self.X_batches_test, self.Y_batches_test = sort_all_classes(self.X_test, self.Y_test, self.batch_size, \\\n",
    "                                                              use_shuffle=True, random_state=current_seed)\n",
    "            yield_batchwise = True\n",
    "        elif strategy == 'max_k_loss':\n",
    "            self._initialize_weights(criterion, device, seed=random_state)\n",
    "            \n",
    "            for iteration in range(num_iterations):\n",
    "                pulled_idxs = weighted_highest_sampling(self.weighted_indices, batch_size=self.batch_size, top_fn=max)\n",
    "                yield self.get_from_idxs(pulled_idxs)\n",
    "                self._update_weights(pulled_idxs, criterion, device)\n",
    "                \n",
    "            # set to False to stop method after this yielding\n",
    "            yield_batchwise = False\n",
    "\n",
    "        elif strategy == 'min_k_loss':\n",
    "            self._initialize_weights(criterion, device, seed=random_state)\n",
    "            \n",
    "            for iteration in range(num_iterations):\n",
    "                pulled_idxs = weighted_highest_sampling(self.weighted_indices, batch_size=self.batch_size, top_fn=min)\n",
    "                yield self.get_from_idxs(pulled_idxs)\n",
    "                self._update_weights(pulled_idxs, criterion, device)\n",
    "                \n",
    "            # set to False to stop method after this yielding\n",
    "            yield_batchwise = False\n",
    "            \n",
    "        if yield_batchwise:\n",
    "            # yield it in batches\n",
    "            batch_idx = 0\n",
    "            X = self.X_batches_train if use_train else self.X_batches_test\n",
    "            Y = self.Y_batches_train if use_train else self.Y_batches_test\n",
    "            X, Y = torch.from_numpy(X), torch.from_numpy(Y)\n",
    "            while batch_idx < len(X):\n",
    "                yield X[batch_idx: batch_idx+self.batch_size], Y[batch_idx: batch_idx+self.batch_size]\n",
    "                batch_idx += self.batch_size\n",
    "            \n",
    "    def get_from_idxs(self, idxs, use_train=True):\n",
    "        if use_train:\n",
    "            X, Y = self.X_train[idxs], self.Y_train[idxs]\n",
    "        else:\n",
    "            X, Y = self.X_test[idxs], self.Y_test[idxs]\n",
    "        return torch.from_numpy(X), torch.from_numpy(Y)\n",
    "    \n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def _initialize_weights(self, criterion, device, seed=None):\n",
    "        assert self.model != None, 'Model needs to be set first!'\n",
    "        \n",
    "        self.weighted_indices = {}\n",
    "        sample_idx = 0\n",
    "        \n",
    "        # using freeze as this is the simplest way of getting data in batches fast\n",
    "        for inputs, targets in dataloader.yield_batches('freeze', random_state=seed, use_train=True):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = self.model(inputs)\n",
    "            losses = criterion(outputs, targets)\n",
    "            for loss in losses:\n",
    "                loss = float(loss.cpu().detach().numpy())\n",
    "                self.weighted_indices[sample_idx] = loss\n",
    "                sample_idx += 1\n",
    "                \n",
    "    def _update_weights(self, idxs, criterion, device, seed=None):\n",
    "        inputs, targets = self.get_from_idxs(idxs)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = self.model(inputs)\n",
    "        losses = criterion(outputs, targets)\n",
    "        for idx, loss in zip(idxs, losses):\n",
    "            self.weighted_indices[idx] = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader()\n",
    "dataloader.download_cifar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet18()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = net.to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.set_model(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "strategy = 'max_k_loss'\n",
    "seed = 42\n",
    "for batch_idx, (inputs, targets) in enumerate(\n",
    "                dataloader.yield_batches(\n",
    "                    strategy, random_state=seed, use_train=True, criterion=criterion, device=device, num_iterations=10)\n",
    "                ):\n",
    "    print(batch_idx)\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    outputs = net(inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
