{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = ['abc', 'def', 'egh', 'ijk', 'lmn', 'opq', 'rst', 'ade', 'des', 'asd', 'a', 'sd', 'sda', 'e']\n",
    "test_targets = [1, 2, 1, 1, 2, 1, 2, 3, 4, 3, 4, 4, 2, 3]\n",
    "\n",
    "expected_output_one_class = ([\n",
    "    ['abc', 'egh', 'ijk'],\n",
    "    ['def', 'lmn', 'rst'],\n",
    "    ['ade', 'asd', 'e'],\n",
    "    ['des', 'a', 'sd'],\n",
    "    ['opq', 'sda']\n",
    "], [\n",
    "    [1, 1, 1], \n",
    "    [2, 2, 2], \n",
    "    [3, 3, 3], \n",
    "    [4, 4, 4], \n",
    "    [1, 2]\n",
    "])\n",
    "\n",
    "expected_output_all_classes = ([\n",
    "    ['opq', 'sda', 'e'],\n",
    "    ['sd', 'ijk', 'rst'],\n",
    "    ['asd', 'a', 'egh'],\n",
    "    ['lmn', 'ade', 'des'],\n",
    "    ['abc', 'def']\n",
    "], [\n",
    "    [1, 2, 3], \n",
    "    [4, 1, 2], \n",
    "    [3, 4, 1], \n",
    "    [2, 3, 4], \n",
    "    [1, 2]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_map_classes_buffer(inputs, targets):\n",
    "    '''\n",
    "    Build a dictionary which has the targets as keys and all the corresponding inputs as values\n",
    "    -- Params:\n",
    "    @inputs: input data for the model\n",
    "    @targets: desired outcome for the model given that it gets @inputs\n",
    "    -- Return: dictionary\n",
    "    '''\n",
    "    # build a dictionary to grab pairs from\n",
    "    map_classes_buffer = defaultdict(list) # stores class -> inputs pairs\n",
    "    list(map(lambda x, y: map_classes_buffer[y].append(x), inputs, targets)) #without list, the writes to the map are not commited\n",
    "    return map_classes_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def batches_one_class(inputs, targets, batch_size=64, use_shuffle=True, random_state=None):\n",
    "    '''\n",
    "    Splits the data (inputs and targets) into as many homogeneous batches as possible,\n",
    "    i.e. that in as many cases as possible, the batches consist of one target only.\n",
    "    -- Params:\n",
    "    @inputs: input data for the model\n",
    "    @targets: desired outcome for the model given that it gets @inputs\n",
    "    @batch_size: number of samples per batch\n",
    "    @use_shuffle: shuffle the data beforehand\n",
    "    -- Return: 2d list\n",
    "    '''\n",
    "    \n",
    "    assert len(inputs) == len(targets), 'Inputs and targets do not have the same size'\n",
    "    \n",
    "    if use_shuffle:        \n",
    "        inputs, targets = shuffle(inputs, targets, random_state=random_state) if random_state else shuffle(inputs, targets)\n",
    "    \n",
    "    input_batches = []\n",
    "    target_batches = []\n",
    "    inputs_buffer = [] # store inputs that don't fit into homogeneous batches anymore\n",
    "    targets_buffer = []\n",
    "    \n",
    "    num_batches = np.ceil(len(inputs) / batch_size)\n",
    "    map_classes_buffer = build_map_classes_buffer(inputs, targets)\n",
    "    \n",
    "    for target in map_classes_buffer.keys():\n",
    "        while len(map_classes_buffer[target]) > 0:\n",
    "            taken_inputs, retrieved_inputs = map_classes_buffer[target][:batch_size], map_classes_buffer[target][batch_size:]\n",
    "            if len(taken_inputs) < batch_size:\n",
    "                inputs_buffer.extend(taken_inputs)\n",
    "                targets_buffer.extend([target for taken_input in taken_inputs])\n",
    "            else:\n",
    "                input_batches.append(taken_inputs)\n",
    "                target_batches.append([target for taken_input in taken_inputs])\n",
    "            map_classes_buffer[target] = retrieved_inputs\n",
    "            \n",
    "    # take missing values\n",
    "    while len(inputs_buffer) > 0:\n",
    "        taken_inputs, inputs_buffer = inputs_buffer[:batch_size], inputs_buffer[batch_size:]\n",
    "        taken_targets, targets_buffer = targets_buffer[:batch_size], targets_buffer[batch_size:]\n",
    "        input_batches.append(taken_inputs)\n",
    "        target_batches.append(taken_targets)\n",
    "                \n",
    "    assert len(input_batches) == num_batches, 'Error in implementation, number of batches is wrong!'\n",
    "    \n",
    "    return input_batches, target_batches\n",
    "    \n",
    "assert batches_one_class(test_inputs, test_targets, batch_size=3, use_shuffle=False) == expected_output_one_class, \\\n",
    "    'Actual and expected outputs differ!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def batches_all_classes(inputs, targets, batch_size=64, use_shuffle=True, random_state=None):\n",
    "    '''\n",
    "    Splits the data (inputs and targets) into as many purely heterogeneous batches as possible,\n",
    "    i.e. that in as many cases as possible, the batches consist of all targets.\n",
    "    -- Params:\n",
    "    @inputs: input data for the model\n",
    "    @targets: desired outcome for the model given that it gets @inputs\n",
    "    @batch_size: number of samples per batch\n",
    "    @use_shuffle: shuffle the data beforehand\n",
    "    -- Return: 2d list\n",
    "    '''\n",
    "    \n",
    "    def try_pop(buffer):\n",
    "        try:\n",
    "            return buffer.pop()\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    \n",
    "    assert len(inputs) == len(targets), 'Inputs and targets do not have the same size'\n",
    "    \n",
    "    if use_shuffle:\n",
    "        inputs, targets = shuffle(inputs, targets, random_state=random_state) if random_state else shuffle(inputs, targets)\n",
    "    \n",
    "    input_batches = []\n",
    "    target_batches = []\n",
    "    inputs_buffer = [] # store inputs that don't fit into purely heterogeneous batches anymore\n",
    "    targets_buffer = []\n",
    "    \n",
    "    num_batches = np.ceil(len(inputs) / batch_size)\n",
    "    map_classes_buffer = build_map_classes_buffer(inputs, targets)\n",
    "    \n",
    "    # check if all targets in map class buffer have at least one element\n",
    "    sorted_inputs = []\n",
    "    sorted_targets = []\n",
    "    copy_map_classes_buffer = deepcopy(map_classes_buffer)\n",
    "    while any(list(map(lambda x: len(map_classes_buffer[x]) > 0, map_classes_buffer))):\n",
    "        taken_targets = list(map(lambda x: x if try_pop(copy_map_classes_buffer[x]) is not None else None,\\\n",
    "                                 copy_map_classes_buffer)) # get one 'column'\n",
    "        taken_inputs = list(map(lambda x: try_pop(map_classes_buffer[x]), map_classes_buffer)) # get one 'column'\n",
    "        sorted_inputs.extend(taken_inputs)\n",
    "        sorted_targets.extend(taken_targets)\n",
    "        \n",
    "    sorted_inputs = [val for val in sorted_inputs if val is not None] # None vals due to try_pop workaround\n",
    "    sorted_targets = [val for val in sorted_targets if val is not None]\n",
    "    while len(sorted_inputs) > batch_size:\n",
    "        input_batches.append(sorted_inputs[:batch_size])\n",
    "        target_batches.append(sorted_targets[:batch_size])\n",
    "        sorted_inputs, sorted_targets = sorted_inputs[batch_size:], sorted_targets[batch_size:]\n",
    "        \n",
    "    if len(sorted_inputs) > 0: # if there are any values left\n",
    "        input_batches.append(sorted_inputs)\n",
    "        target_batches.append(sorted_targets)\n",
    "            \n",
    "    assert len(input_batches) == num_batches, 'Error in implementation, number of batches is wrong!'\n",
    "    \n",
    "    return input_batches, target_batches \n",
    "\n",
    "assert batches_all_classes(test_inputs, test_targets, batch_size=3, use_shuffle=False) == expected_output_all_classes, \\\n",
    "    'Actual and expected outputs differ!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from utils.data_utils import DataLoader\n",
    "dataloader = DataLoader(batch_sizes={'train': 64, 'test': 64}, shuffle={'train': False, 'test': False})\n",
    "dataloader.download_cifar()\n",
    "trainloader, testloader = dataloader.get_loaders()\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    _inputs = inputs\n",
    "    _targets = targets\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6, 7, 0, 4, 9,\n",
       "        5, 2, 4, 0, 9, 6, 6, 5, 4, 5, 9, 2, 4, 1, 9, 5, 4, 6, 5, 6, 0, 9, 3, 9,\n",
       "        7, 6, 9, 8, 0, 3, 8, 8, 7, 7, 4, 6, 7, 3, 6, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_targets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
