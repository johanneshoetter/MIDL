{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = ['abc', 'def', 'egh', 'ijk', 'lmn', 'opq', 'rst', 'ade', 'des', 'asd', 'a', 'sd', 'sda', 'e']\n",
    "test_targets = [1, 2, 1, 1, 2, 1, 2, 3, 4, 3, 4, 4, 2, 3]\n",
    "\n",
    "expected_output_one_class = \\\n",
    "(['abc','egh','ijk','def','lmn','rst','ade','asd','e','des','a','sd','opq','sda'],\n",
    " [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 1, 2])\n",
    "\n",
    "expected_output_all_classes = \\\n",
    "(['opq','sda','e','sd','ijk','rst','asd','a','egh','lmn','ade','des','abc','def'],\n",
    " [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_map_classes_buffer(inputs, targets):\n",
    "    '''\n",
    "    Build a dictionary which has the targets as keys and all the corresponding inputs as values\n",
    "    -- Params:\n",
    "    @inputs: input data for the model\n",
    "    @targets: desired outcome for the model given that it gets @inputs\n",
    "    -- Return: dictionary\n",
    "    '''\n",
    "    # build a dictionary to grab pairs from\n",
    "    map_classes_buffer = defaultdict(list) # stores class -> inputs pairs\n",
    "    list(map(lambda x, y: map_classes_buffer[y].append(x), inputs, targets)) #without list, the writes to the map are not commited\n",
    "    return map_classes_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def sort_one_class(inputs, targets, batch_size, use_shuffle=True, random_state=None):\n",
    "    '''\n",
    "    Splits the data (inputs and targets) into as many homogeneous batches as possible,\n",
    "    i.e. that in as many cases as possible, the batches consist of one target only.\n",
    "    -- Params:\n",
    "    @inputs: input data for the model\n",
    "    @targets: desired outcome for the model given that it gets @inputs\n",
    "    @batch_size: number of samples per batch\n",
    "    @use_shuffle: shuffle the data beforehand\n",
    "    -- Return: one 2d list for the inputs, one 2d list for the targets\n",
    "    '''\n",
    "    \n",
    "    assert len(inputs) == len(targets), 'Inputs and targets do not have the same size'\n",
    "    \n",
    "    if use_shuffle:        \n",
    "        inputs, targets = shuffle(inputs, targets, random_state=random_state)\n",
    "    \n",
    "    input_batches = []\n",
    "    target_batches = []\n",
    "    inputs_buffer = [] # store inputs that don't fit into homogeneous batches anymore\n",
    "    targets_buffer = []\n",
    "    \n",
    "    num_batches = np.ceil(len(inputs) / batch_size)\n",
    "    map_classes_buffer = build_map_classes_buffer(inputs, targets)\n",
    "    \n",
    "    for target in map_classes_buffer.keys():\n",
    "        while len(map_classes_buffer[target]) > 0:\n",
    "            taken_inputs, retrieved_inputs = map_classes_buffer[target][:batch_size], map_classes_buffer[target][batch_size:]\n",
    "            if len(taken_inputs) < batch_size:\n",
    "                inputs_buffer.extend(taken_inputs)\n",
    "                targets_buffer.extend([target for taken_input in taken_inputs])\n",
    "            else:\n",
    "                input_batches.extend(taken_inputs)\n",
    "                target_batches.extend([target for taken_input in taken_inputs])\n",
    "            map_classes_buffer[target] = retrieved_inputs\n",
    "            \n",
    "    # take missing values\n",
    "    while len(inputs_buffer) > 0:\n",
    "        taken_inputs, inputs_buffer = inputs_buffer[:batch_size], inputs_buffer[batch_size:]\n",
    "        taken_targets, targets_buffer = targets_buffer[:batch_size], targets_buffer[batch_size:]\n",
    "        input_batches.extend(taken_inputs)\n",
    "        target_batches.extend(taken_targets)\n",
    "                \n",
    "    return input_batches, target_batches\n",
    " \n",
    "sort_one_class(test_inputs, test_targets, batch_size=3, use_shuffle=False)\n",
    "assert sort_one_class(test_inputs, test_targets, batch_size=3, use_shuffle=False) == expected_output_one_class, \\\n",
    "    'Actual and expected outputs differ!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def sort_all_classes(inputs, targets, batch_size, use_shuffle=True, random_state=None):\n",
    "    '''\n",
    "    Splits the data (inputs and targets) into as many purely heterogeneous batches as possible,\n",
    "    i.e. that in as many cases as possible, the batches consist of all targets.\n",
    "    -- Params:\n",
    "    @inputs: input data for the model\n",
    "    @targets: desired outcome for the model given that it gets @inputs\n",
    "    @batch_size: number of samples per batch\n",
    "    @use_shuffle: shuffle the data beforehand\n",
    "    -- Return: one 2d list for the inputs, one 2d list for the targets\n",
    "    '''\n",
    "    \n",
    "    def try_pop(buffer):\n",
    "        try:\n",
    "            return buffer.pop()\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    \n",
    "    assert len(inputs) == len(targets), 'Inputs and targets do not have the same size'\n",
    "    \n",
    "    if use_shuffle:\n",
    "        inputs, targets = shuffle(inputs, targets, random_state=random_state)\n",
    "    \n",
    "    input_batches = []\n",
    "    target_batches = []\n",
    "    inputs_buffer = [] # store inputs that don't fit into purely heterogeneous batches anymore\n",
    "    targets_buffer = []\n",
    "    \n",
    "    num_batches = np.ceil(len(inputs) / batch_size)\n",
    "    map_classes_buffer = build_map_classes_buffer(inputs, targets)\n",
    "    \n",
    "    # check if all targets in map class buffer have at least one element\n",
    "    sorted_inputs = []\n",
    "    sorted_targets = []\n",
    "    copy_map_classes_buffer = deepcopy(map_classes_buffer)\n",
    "    while any(list(map(lambda x: len(map_classes_buffer[x]) > 0, map_classes_buffer))):\n",
    "        taken_targets = list(map(lambda x: x if try_pop(copy_map_classes_buffer[x]) is not None else None,\\\n",
    "                                 copy_map_classes_buffer)) # get one 'column'\n",
    "        taken_inputs = list(map(lambda x: try_pop(map_classes_buffer[x]), map_classes_buffer)) # get one 'column'\n",
    "        sorted_inputs.extend(taken_inputs)\n",
    "        sorted_targets.extend(taken_targets)\n",
    "        \n",
    "    sorted_inputs = [val for val in sorted_inputs if val is not None] # None vals due to try_pop workaround\n",
    "    sorted_targets = [val for val in sorted_targets if val is not None]\n",
    "    while len(sorted_inputs) > batch_size:\n",
    "        input_batches.extend(sorted_inputs[:batch_size])\n",
    "        target_batches.extend(sorted_targets[:batch_size])\n",
    "        sorted_inputs, sorted_targets = sorted_inputs[batch_size:], sorted_targets[batch_size:]\n",
    "        \n",
    "    if len(sorted_inputs) > 0: # if there are any values left\n",
    "        input_batches.extend(sorted_inputs)\n",
    "        target_batches.extend(sorted_targets)\n",
    "                \n",
    "    return input_batches, target_batches \n",
    "\n",
    "sort_all_classes(test_inputs, test_targets, batch_size=3, use_shuffle=False)\n",
    "assert sort_all_classes(test_inputs, test_targets, batch_size=3, use_shuffle=False) == expected_output_all_classes, \\\n",
    "    'Actual and expected outputs differ!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['c', 'h', 'f'], [0, 1, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "def weighted_random_sampling(inputs, targets, weighted_indices, batch_size=3):\n",
    "    '''\n",
    "    In weighted random sampling (WRS) the items are weighted and the probability of \n",
    "    each item to be selected is determined by its relative weight.\n",
    "    -- Params:\n",
    "    @inputs: input data for the model\n",
    "    @targets: desired outcome for the model given that it gets @inputs\n",
    "    @weighted_indices: dictionary containing which index of the samples has which probability\n",
    "    @batch_size: number of samples per batch\n",
    "    -- Return: one batch of the inputs, one batch of the targets\n",
    "    '''\n",
    "    eps = 0.0000001 # small threshold to accept rounding errors when comparing float values\n",
    "    \n",
    "    assert len(inputs) == len(targets), 'The number of inputs to pull from must fit the number of targets'\n",
    "    assert len(inputs) == len(weighted_indices), 'The number of inputs to pull from must fit the given weighted indices'\n",
    "    assert type(inputs) == list, 'The inputs to pull from must be given as a list'\n",
    "    assert type(weighted_indices) == dict, 'The weighted indices must be given as a dictionary'\n",
    "    assert sum(weighted_indices.values()) < 1 + eps and sum(weighted_indices.values()) > 1 - eps, \\\n",
    "        'The sum of the values for the input must add up to 1.'\n",
    "    \n",
    "    pulled_samples = []\n",
    "    pulled_targets = []\n",
    "    for _ in range(batch_size):\n",
    "        # calculate the weights of an element to be pulled\n",
    "        # must be recalculated each round, as the values have to add up to 1\n",
    "        pulled_index = np.random.choice(list(weighted_indices.keys()), p = list(weighted_indices.values()))\n",
    "        weighted_indices[pulled_index] = 0\n",
    "        sum_of_values = sum(weighted_indices.values())\n",
    "        for index, weight in weighted_indices.items():\n",
    "            weighted_indices[index] = weight / sum_of_values\n",
    "        pulled_samples.append(inputs[pulled_index])\n",
    "        pulled_targets.append(targets[pulled_index])\n",
    "    return pulled_samples, pulled_targets\n",
    "    \n",
    "    \n",
    "inputs = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'] # will be tensors\n",
    "targets = [0, 0, 0, 1, 1, 0, 1, 1]\n",
    "weighted_indices = {\n",
    "    0: 0.3,\n",
    "    1: 0.05,\n",
    "    2: 0.2,\n",
    "    3: 0.05,\n",
    "    4: 0.05,\n",
    "    5: 0.15,\n",
    "    6: 0.05,\n",
    "    7: 0.15\n",
    "}\n",
    "\n",
    "weighted_random_sampling(inputs, targets, weighted_indices, batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
