{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = ['abc', 'def', 'egh', 'ijk', 'lmn', 'opq', 'rst', 'ade', 'des', 'asd', 'a', 'sd', 'sda', 'e']\n",
    "test_targets = [1, 2, 1, 1, 2, 1, 2, 3, 4, 3, 4, 4, 2, 3]\n",
    "\n",
    "expected_output_one_class = \\\n",
    "(['abc','egh','ijk','def','lmn','rst','ade','asd','e','des','a','sd','opq','sda'],\n",
    " [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 1, 2])\n",
    "\n",
    "expected_output_all_classes = \\\n",
    "(['opq','sda','e','sd','ijk','rst','asd','a','egh','lmn','ade','des','abc','def'],\n",
    " [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_map_classes_buffer(inputs, targets):\n",
    "    '''\n",
    "    Build a dictionary which has the targets as keys and all the corresponding inputs as values\n",
    "    -- Params:\n",
    "    @inputs: input data for the model\n",
    "    @targets: desired outcome for the model given that it gets @inputs\n",
    "    -- Return: dictionary\n",
    "    '''\n",
    "    # build a dictionary to grab pairs from\n",
    "    map_classes_buffer = defaultdict(list) # stores class -> inputs pairs\n",
    "    list(map(lambda x, y: map_classes_buffer[y].append(x), inputs, targets)) #without list, the writes to the map are not commited\n",
    "    return map_classes_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def sort_one_class(inputs, targets, batch_size, use_shuffle=True, random_state=None):\n",
    "    '''\n",
    "    Splits the data (inputs and targets) into as many homogeneous batches as possible,\n",
    "    i.e. that in as many cases as possible, the batches consist of one target only.\n",
    "    -- Params:\n",
    "    @inputs: input data for the model\n",
    "    @targets: desired outcome for the model given that it gets @inputs\n",
    "    @batch_size: number of samples per batch\n",
    "    @use_shuffle: shuffle the data beforehand\n",
    "    -- Return: one 2d list for the inputs, one 2d list for the targets\n",
    "    '''\n",
    "    \n",
    "    assert len(inputs) == len(targets), 'Inputs and targets do not have the same size'\n",
    "    \n",
    "    if use_shuffle:        \n",
    "        inputs, targets = shuffle(inputs, targets, random_state=random_state)\n",
    "    \n",
    "    input_batches = []\n",
    "    target_batches = []\n",
    "    inputs_buffer = [] # store inputs that don't fit into homogeneous batches anymore\n",
    "    targets_buffer = []\n",
    "    \n",
    "    num_batches = np.ceil(len(inputs) / batch_size)\n",
    "    map_classes_buffer = build_map_classes_buffer(inputs, targets)\n",
    "    \n",
    "    for target in map_classes_buffer.keys():\n",
    "        while len(map_classes_buffer[target]) > 0:\n",
    "            taken_inputs, retrieved_inputs = map_classes_buffer[target][:batch_size], map_classes_buffer[target][batch_size:]\n",
    "            if len(taken_inputs) < batch_size:\n",
    "                inputs_buffer.extend(taken_inputs)\n",
    "                targets_buffer.extend([target for taken_input in taken_inputs])\n",
    "            else:\n",
    "                input_batches.extend(taken_inputs)\n",
    "                target_batches.extend([target for taken_input in taken_inputs])\n",
    "            map_classes_buffer[target] = retrieved_inputs\n",
    "            \n",
    "    # take missing values\n",
    "    while len(inputs_buffer) > 0:\n",
    "        taken_inputs, inputs_buffer = inputs_buffer[:batch_size], inputs_buffer[batch_size:]\n",
    "        taken_targets, targets_buffer = targets_buffer[:batch_size], targets_buffer[batch_size:]\n",
    "        input_batches.extend(taken_inputs)\n",
    "        target_batches.extend(taken_targets)\n",
    "                \n",
    "    return input_batches, target_batches\n",
    " \n",
    "sort_one_class(test_inputs, test_targets, batch_size=3, use_shuffle=False)\n",
    "assert sort_one_class(test_inputs, test_targets, batch_size=3, use_shuffle=False) == expected_output_one_class, \\\n",
    "    'Actual and expected outputs differ!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def sort_all_classes(inputs, targets, batch_size, use_shuffle=True, random_state=None):\n",
    "    '''\n",
    "    Splits the data (inputs and targets) into as many purely heterogeneous batches as possible,\n",
    "    i.e. that in as many cases as possible, the batches consist of all targets.\n",
    "    -- Params:\n",
    "    @inputs: input data for the model\n",
    "    @targets: desired outcome for the model given that it gets @inputs\n",
    "    @batch_size: number of samples per batch\n",
    "    @use_shuffle: shuffle the data beforehand\n",
    "    -- Return: one 2d list for the inputs, one 2d list for the targets\n",
    "    '''\n",
    "    \n",
    "    def try_pop(buffer):\n",
    "        try:\n",
    "            return buffer.pop()\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    \n",
    "    assert len(inputs) == len(targets), 'Inputs and targets do not have the same size'\n",
    "    \n",
    "    if use_shuffle:\n",
    "        inputs, targets = shuffle(inputs, targets, random_state=random_state)\n",
    "    \n",
    "    input_batches = []\n",
    "    target_batches = []\n",
    "    inputs_buffer = [] # store inputs that don't fit into purely heterogeneous batches anymore\n",
    "    targets_buffer = []\n",
    "    \n",
    "    num_batches = np.ceil(len(inputs) / batch_size)\n",
    "    map_classes_buffer = build_map_classes_buffer(inputs, targets)\n",
    "    \n",
    "    # check if all targets in map class buffer have at least one element\n",
    "    sorted_inputs = []\n",
    "    sorted_targets = []\n",
    "    copy_map_classes_buffer = deepcopy(map_classes_buffer)\n",
    "    while any(list(map(lambda x: len(map_classes_buffer[x]) > 0, map_classes_buffer))):\n",
    "        taken_targets = list(map(lambda x: x if try_pop(copy_map_classes_buffer[x]) is not None else None,\\\n",
    "                                 copy_map_classes_buffer)) # get one 'column'\n",
    "        taken_inputs = list(map(lambda x: try_pop(map_classes_buffer[x]), map_classes_buffer)) # get one 'column'\n",
    "        sorted_inputs.extend(taken_inputs)\n",
    "        sorted_targets.extend(taken_targets)\n",
    "        \n",
    "    sorted_inputs = [val for val in sorted_inputs if val is not None] # None vals due to try_pop workaround\n",
    "    sorted_targets = [val for val in sorted_targets if val is not None]\n",
    "    while len(sorted_inputs) > batch_size:\n",
    "        input_batches.extend(sorted_inputs[:batch_size])\n",
    "        target_batches.extend(sorted_targets[:batch_size])\n",
    "        sorted_inputs, sorted_targets = sorted_inputs[batch_size:], sorted_targets[batch_size:]\n",
    "        \n",
    "    if len(sorted_inputs) > 0: # if there are any values left\n",
    "        input_batches.extend(sorted_inputs)\n",
    "        target_batches.extend(sorted_targets)\n",
    "                \n",
    "    return input_batches, target_batches \n",
    "\n",
    "sort_all_classes(test_inputs, test_targets, batch_size=3, use_shuffle=False)\n",
    "assert sort_all_classes(test_inputs, test_targets, batch_size=3, use_shuffle=False) == expected_output_all_classes, \\\n",
    "    'Actual and expected outputs differ!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 2]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weighted_random_sampling(weighted_indices, batch_size=3, top_fn=max, random_state=None):\n",
    "    '''\n",
    "    In weighted random sampling (WRS) the items are weighted and the probability of \n",
    "    each item to be selected is determined by its relative weight.\n",
    "    -- Params:\n",
    "    @weighted_indices: dictionary containing which index of the samples has which probability\n",
    "    @batch_size: number of samples per batch\n",
    "    -- Return: indices of the pulled inputs\n",
    "    '''\n",
    "    \n",
    "    assert type(weighted_indices) == dict, 'The weighted indices must be given as a dictionary'  \n",
    "\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    pulled_indices = []\n",
    "    for _ in range(batch_size):\n",
    "        pulled_index = np.random.choice(list(weighted_indices.keys()), p = list(weighted_indices.values()))\n",
    "        weighted_indices[pulled_index] = 0\n",
    "        sum_of_values = sum(weighted_indices.values())\n",
    "        for index, weight in weighted_indices.items():\n",
    "            weighted_indices[index] = weight / sum_of_values\n",
    "        pulled_indices.append(pulled_index)\n",
    "    return pulled_indices\n",
    "\n",
    "weighted_indices = {\n",
    "    0: 0.3,\n",
    "    1: 0.05,\n",
    "    2: 0.2,\n",
    "    3: 0.05,\n",
    "    4: 0.05,\n",
    "    5: 0.15,\n",
    "    6: 0.05,\n",
    "    7: 0.15\n",
    "}\n",
    "\n",
    "weighted_random_sampling(weighted_indices, batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['c', 'h', 'f'], [0, 1, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "def weighted_random_sampling(inputs, targets, weighted_indices, batch_size=3):\n",
    "    '''\n",
    "    In weighted random sampling (WRS) the items are weighted and the probability of \n",
    "    each item to be selected is determined by its relative weight.\n",
    "    -- Params:\n",
    "    @inputs: input data for the model\n",
    "    @targets: desired outcome for the model given that it gets @inputs\n",
    "    @weighted_indices: dictionary containing which index of the samples has which probability\n",
    "    @batch_size: number of samples per batch\n",
    "    -- Return: one batch of the inputs, one batch of the targets\n",
    "    '''\n",
    "    eps = 0.0000001 # small threshold to accept rounding errors when comparing float values\n",
    "    \n",
    "    assert len(inputs) == len(targets), 'The number of inputs to pull from must fit the number of targets'\n",
    "    assert len(inputs) == len(weighted_indices), 'The number of inputs to pull from must fit the given weighted indices'\n",
    "    assert type(weighted_indices) == dict, 'The weighted indices must be given as a dictionary'\n",
    "    assert sum(weighted_indices.values()) < 1 + eps and sum(weighted_indices.values()) > 1 - eps, \\\n",
    "        'The sum of the values for the input must add up to 1.'\n",
    "    \n",
    "    pulled_samples = []\n",
    "    pulled_targets = []\n",
    "    for _ in range(batch_size):\n",
    "        # calculate the weights of an element to be pulled\n",
    "        # must be recalculated each round, as the values have to add up to 1\n",
    "        pulled_index = np.random.choice(list(weighted_indices.keys()), p = list(weighted_indices.values()))\n",
    "        weighted_indices[pulled_index] = 0\n",
    "        sum_of_values = sum(weighted_indices.values())\n",
    "        for index, weight in weighted_indices.items():\n",
    "            weighted_indices[index] = weight / sum_of_values\n",
    "        pulled_samples.append(inputs[pulled_index])\n",
    "        pulled_targets.append(targets[pulled_index])\n",
    "    return pulled_samples, pulled_targets\n",
    "    \n",
    "    \n",
    "inputs = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'] # will be tensors\n",
    "targets = [0, 0, 0, 1, 1, 0, 1, 1]\n",
    "weighted_indices = {\n",
    "    0: 0.3,\n",
    "    1: 0.05,\n",
    "    2: 0.2,\n",
    "    3: 0.05,\n",
    "    4: 0.05,\n",
    "    5: 0.15,\n",
    "    6: 0.05,\n",
    "    7: 0.15\n",
    "}\n",
    "\n",
    "weighted_random_sampling(inputs, targets, weighted_indices, batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a', 'c'], [0, 0], [0, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "def weighted_highest_sampling(inputs, targets, weighted_indices, batch_size=3, top_fn=max):\n",
    "    '''\n",
    "    In weighted random sampling (WRS) the items are weighted and the probability of \n",
    "    each item to be selected is determined by its relative weight.\n",
    "    -- Params:\n",
    "    @inputs: input data for the model\n",
    "    @targets: desired outcome for the model given that it gets @inputs\n",
    "    @weighted_indices: dictionary containing which index of the samples has which probability\n",
    "    @batch_size: number of samples per batch\n",
    "    -- Return: one batch of the inputs, one batch of the targets\n",
    "    '''\n",
    "    \n",
    "    assert len(inputs) == len(targets), 'The number of inputs to pull from must fit the number of targets'\n",
    "    assert len(inputs) == len(weighted_indices), 'The number of inputs to pull from must fit the given weighted indices'\n",
    "    assert type(weighted_indices) == dict, 'The weighted indices must be given as a dictionary'\n",
    "      \n",
    "    pulled_samples = []\n",
    "    pulled_targets = []\n",
    "    pulled_indices = []\n",
    "    for _ in range(batch_size):\n",
    "        pulled_index = top_fn(weighted_indices.items(), key=operator.itemgetter(1))[0]            \n",
    "        weighted_indices[pulled_index] = 0\n",
    "        pulled_samples.append(inputs[pulled_index])\n",
    "        pulled_targets.append(targets[pulled_index])\n",
    "        pulled_indices.append(pulled_index)\n",
    "    return pulled_samples, pulled_targets, pulled_indices\n",
    "    \n",
    "    \n",
    "inputs = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'] # will be tensors\n",
    "targets = [0, 0, 0, 1, 1, 0, 1, 1]\n",
    "weighted_indices = {\n",
    "    0: 0.3,\n",
    "    1: 0.05,\n",
    "    2: 0.2,\n",
    "    3: 0.05,\n",
    "    4: 0.05,\n",
    "    5: 0.15,\n",
    "    6: 0.05,\n",
    "    7: 0.15\n",
    "}\n",
    "\n",
    "weighted_highest_sampling(inputs, targets, weighted_indices, batch_size=2, top_fn=max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weighted_highest_sampling_per_class(weighted_indices_per_class, batch_size=3, top_fn=max):\n",
    "    '''\n",
    "    In weighted random sampling (WRS) the items are weighted and the probability of \n",
    "    each item to be selected is determined by its relative weight.\n",
    "    -- Params:\n",
    "    @weighted_indices: dictionary containing which index of the samples has which probability\n",
    "    @batch_size: number of samples per batch\n",
    "    -- Return: indices of the pulled inputs\n",
    "    '''\n",
    "    \n",
    "    assert type(weighted_indices) == dict, 'The weighted indices must be given as a dictionary'  \n",
    "\n",
    "    pulled_indices = []\n",
    "    key_range = list(weighted_indices_per_class.keys())\n",
    "    cur_key_idx = 0\n",
    "    for _ in range(batch_size):\n",
    "        key = key_range[cur_key_idx % len(key_range)]\n",
    "        cur_key_idx += 1\n",
    "        pulled_index = top_fn(weighted_indices_per_class[key].items(), key=operator.itemgetter(1))[0]            \n",
    "        weighted_indices_per_class[key][pulled_index] = 0\n",
    "        pulled_indices.append(pulled_index)\n",
    "    return pulled_indices\n",
    "    \n",
    "inputs = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'] # will be tensors\n",
    "targets = [0, 0, 0, 1, 1, 0, 1, 1]\n",
    "weighted_indices_per_class = {\n",
    "    0: {\n",
    "        0: 0.2,\n",
    "        1: 0.4,\n",
    "        2: 0.1,\n",
    "        3: 0.3\n",
    "    },\n",
    "    1: {\n",
    "        4: 0.6,\n",
    "        5: 0.2,\n",
    "        6: 0.1,\n",
    "        7: 0.1\n",
    "    }\n",
    "}\n",
    "\n",
    "weighted_highest_sampling_per_class(weighted_indices_per_class, batch_size=3, top_fn=max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10000\n",
    "batch_size = 64\n",
    "# prepare test data\n",
    "X = np.random.rand(num_samples, 32, 32)\n",
    "Y = np.random.randint(10, size=num_samples)\n",
    "weighted_indices = {index: abs(np.random.normal(loc=0, scale=1)) for index in range(len(X))}\n",
    "sum_indices = sum(weighted_indices.values())\n",
    "for key, value in weighted_indices.items():\n",
    "    weighted_indices[key] /= sum_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_copy, Y_copy, wi_copy = X.copy(), Y.copy(), weighted_indices.copy()\n",
    "start = timeit.default_timer()\n",
    "_ = shuffle(X_copy, Y_copy)\n",
    "stop = timeit.default_timer()\n",
    "times['shuffle'] = stop - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_copy, Y_copy, wi_copy = X.copy(), Y.copy(), weighted_indices.copy()\n",
    "start = timeit.default_timer()\n",
    "_ = weighted_highest_sampling(X_copy, Y_copy, wi_copy,  batch_size=batch_size)\n",
    "stop = timeit.default_timer()\n",
    "times['whs'] = stop - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_copy, Y_copy, wi_copy = X.copy(), Y.copy(), weighted_indices.copy()\n",
    "start = timeit.default_timer()\n",
    "_ = weighted_random_sampling(X_copy, Y_copy, wi_copy,  batch_size=batch_size)\n",
    "stop = timeit.default_timer()\n",
    "times['wrs'] = stop - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_copy, Y_copy, wi_copy = X.copy(), Y.copy(), weighted_indices.copy()\n",
    "start = timeit.default_timer()\n",
    "_ = sort_one_class(X_copy, Y_copy, batch_size)\n",
    "stop = timeit.default_timer()\n",
    "times['homogeneous'] = stop - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_copy, Y_copy, wi_copy = X.copy(), Y.copy(), weighted_indices.copy()\n",
    "start = timeit.default_timer()\n",
    "_ = sort_all_classes(X_copy, Y_copy, batch_size)\n",
    "stop = timeit.default_timer()\n",
    "times['heterogeneous'] = stop - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(times.keys(), times.values())\n",
    "plt.xticks(rotation=30)\n",
    "plt.ylabel('seconds needed')\n",
    "plt.title('Batching for {} samples'.format(num_samples))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
