{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = ['abc', 'def', 'egh', 'ijk', 'lmn', 'opq', 'rst', 'ade', 'des', 'asd', 'a', 'sd', 'sda', 'e']\n",
    "test_targets = [1, 2, 1, 1, 2, 1, 2, 3, 4, 3, 4, 4, 2, 3]\n",
    "\n",
    "expected_output_one_class = \\\n",
    "(['abc','egh','ijk','def','lmn','rst','ade','asd','e','des','a','sd','opq','sda'],\n",
    " [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 1, 2])\n",
    "\n",
    "expected_output_all_classes = \\\n",
    "(['opq','sda','e','sd','ijk','rst','asd','a','egh','lmn','ade','des','abc','def'],\n",
    " [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_map_classes_buffer(inputs, targets):\n",
    "    '''\n",
    "    Build a dictionary which has the targets as keys and all the corresponding inputs as values\n",
    "    -- Params:\n",
    "    @inputs: input data for the model\n",
    "    @targets: desired outcome for the model given that it gets @inputs\n",
    "    -- Return: dictionary\n",
    "    '''\n",
    "    # build a dictionary to grab pairs from\n",
    "    map_classes_buffer = defaultdict(list) # stores class -> inputs pairs\n",
    "    list(map(lambda x, y: map_classes_buffer[y].append(x), inputs, targets)) #without list, the writes to the map are not commited\n",
    "    return map_classes_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def sort_one_class(inputs, targets, batch_size, use_shuffle=True, random_state=None):\n",
    "    '''\n",
    "    Splits the data (inputs and targets) into as many homogeneous batches as possible,\n",
    "    i.e. that in as many cases as possible, the batches consist of one target only.\n",
    "    -- Params:\n",
    "    @inputs: input data for the model\n",
    "    @targets: desired outcome for the model given that it gets @inputs\n",
    "    @batch_size: number of samples per batch\n",
    "    @use_shuffle: shuffle the data beforehand\n",
    "    -- Return: 2d list\n",
    "    '''\n",
    "    \n",
    "    assert len(inputs) == len(targets), 'Inputs and targets do not have the same size'\n",
    "    \n",
    "    if use_shuffle:        \n",
    "        inputs, targets = shuffle(inputs, targets, random_state=random_state)\n",
    "    \n",
    "    input_batches = []\n",
    "    target_batches = []\n",
    "    inputs_buffer = [] # store inputs that don't fit into homogeneous batches anymore\n",
    "    targets_buffer = []\n",
    "    \n",
    "    num_batches = np.ceil(len(inputs) / batch_size)\n",
    "    map_classes_buffer = build_map_classes_buffer(inputs, targets)\n",
    "    \n",
    "    for target in map_classes_buffer.keys():\n",
    "        while len(map_classes_buffer[target]) > 0:\n",
    "            taken_inputs, retrieved_inputs = map_classes_buffer[target][:batch_size], map_classes_buffer[target][batch_size:]\n",
    "            if len(taken_inputs) < batch_size:\n",
    "                inputs_buffer.extend(taken_inputs)\n",
    "                targets_buffer.extend([target for taken_input in taken_inputs])\n",
    "            else:\n",
    "                input_batches.extend(taken_inputs)\n",
    "                target_batches.extend([target for taken_input in taken_inputs])\n",
    "            map_classes_buffer[target] = retrieved_inputs\n",
    "            \n",
    "    # take missing values\n",
    "    while len(inputs_buffer) > 0:\n",
    "        taken_inputs, inputs_buffer = inputs_buffer[:batch_size], inputs_buffer[batch_size:]\n",
    "        taken_targets, targets_buffer = targets_buffer[:batch_size], targets_buffer[batch_size:]\n",
    "        input_batches.extend(taken_inputs)\n",
    "        target_batches.extend(taken_targets)\n",
    "                \n",
    "    return input_batches, target_batches\n",
    " \n",
    "sort_one_class(test_inputs, test_targets, batch_size=3, use_shuffle=False)\n",
    "assert sort_one_class(test_inputs, test_targets, batch_size=3, use_shuffle=False) == expected_output_one_class, \\\n",
    "    'Actual and expected outputs differ!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def sort_all_classes(inputs, targets, batch_size, use_shuffle=True, random_state=None):\n",
    "    '''\n",
    "    Splits the data (inputs and targets) into as many purely heterogeneous batches as possible,\n",
    "    i.e. that in as many cases as possible, the batches consist of all targets.\n",
    "    -- Params:\n",
    "    @inputs: input data for the model\n",
    "    @targets: desired outcome for the model given that it gets @inputs\n",
    "    @batch_size: number of samples per batch\n",
    "    @use_shuffle: shuffle the data beforehand\n",
    "    -- Return: 2d list\n",
    "    '''\n",
    "    \n",
    "    def try_pop(buffer):\n",
    "        try:\n",
    "            return buffer.pop()\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    \n",
    "    assert len(inputs) == len(targets), 'Inputs and targets do not have the same size'\n",
    "    \n",
    "    if use_shuffle:\n",
    "        inputs, targets = shuffle(inputs, targets, random_state=random_state)\n",
    "    \n",
    "    input_batches = []\n",
    "    target_batches = []\n",
    "    inputs_buffer = [] # store inputs that don't fit into purely heterogeneous batches anymore\n",
    "    targets_buffer = []\n",
    "    \n",
    "    num_batches = np.ceil(len(inputs) / batch_size)\n",
    "    map_classes_buffer = build_map_classes_buffer(inputs, targets)\n",
    "    \n",
    "    # check if all targets in map class buffer have at least one element\n",
    "    sorted_inputs = []\n",
    "    sorted_targets = []\n",
    "    copy_map_classes_buffer = deepcopy(map_classes_buffer)\n",
    "    while any(list(map(lambda x: len(map_classes_buffer[x]) > 0, map_classes_buffer))):\n",
    "        taken_targets = list(map(lambda x: x if try_pop(copy_map_classes_buffer[x]) is not None else None,\\\n",
    "                                 copy_map_classes_buffer)) # get one 'column'\n",
    "        taken_inputs = list(map(lambda x: try_pop(map_classes_buffer[x]), map_classes_buffer)) # get one 'column'\n",
    "        sorted_inputs.extend(taken_inputs)\n",
    "        sorted_targets.extend(taken_targets)\n",
    "        \n",
    "    sorted_inputs = [val for val in sorted_inputs if val is not None] # None vals due to try_pop workaround\n",
    "    sorted_targets = [val for val in sorted_targets if val is not None]\n",
    "    while len(sorted_inputs) > batch_size:\n",
    "        input_batches.extend(sorted_inputs[:batch_size])\n",
    "        target_batches.extend(sorted_targets[:batch_size])\n",
    "        sorted_inputs, sorted_targets = sorted_inputs[batch_size:], sorted_targets[batch_size:]\n",
    "        \n",
    "    if len(sorted_inputs) > 0: # if there are any values left\n",
    "        input_batches.extend(sorted_inputs)\n",
    "        target_batches.extend(sorted_targets)\n",
    "                \n",
    "    return input_batches, target_batches \n",
    "\n",
    "sort_all_classes(test_inputs, test_targets, batch_size=3, use_shuffle=False)\n",
    "assert sort_all_classes(test_inputs, test_targets, batch_size=3, use_shuffle=False) == expected_output_all_classes, \\\n",
    "    'Actual and expected outputs differ!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['c', 'h', 'f'], [0, 1, 0])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "def weighted_random_sampling(inputs, targets, weighted_indices, m=3):\n",
    "    eps = 0.0000001 # small threshold to accept rounding errors when comparing float values\n",
    "    \n",
    "    assert len(inputs) == len(targets), 'The number of inputs to pull from must fit the number of targets'\n",
    "    assert len(inputs) == len(weighted_indices), 'The number of inputs to pull from must fit the given weighted indices'\n",
    "    assert type(inputs) == list, 'The inputs to pull from must be given as a list'\n",
    "    assert type(weighted_indices) == dict, 'The weighted indices must be given as a dictionary'\n",
    "    assert sum(weighted_indices.values()) < 1 + eps and sum(weighted_indices.values()) > 1 - eps, \\\n",
    "        'The sum of the values for the input must add up to 1.'\n",
    "    \n",
    "    pulled_samples = []\n",
    "    pulled_targets = []\n",
    "    for k in range(m):\n",
    "        # calculate the weights of an element to be pulled\n",
    "        # must be recalculated each round, as the values have to add up to 1\n",
    "        pulled_index = np.random.choice(list(weighted_indices.keys()), p = list(weighted_indices.values()))\n",
    "        weighted_indices[pulled_index] = 0\n",
    "        sum_of_values = sum(weighted_indices.values())\n",
    "        for index, weight in weighted_indices.items():\n",
    "            weighted_indices[index] = weight / sum_of_values\n",
    "        pulled_samples.append(inputs[pulled_index])\n",
    "        pulled_targets.append(targets[pulled_index])\n",
    "    return pulled_samples, pulled_targets\n",
    "    \n",
    "    \n",
    "inputs = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'] # will be tensors\n",
    "targets = [0, 0, 0, 1, 1, 0, 1, 1]\n",
    "weighted_indices = {\n",
    "    0: 0.3,\n",
    "    1: 0.05,\n",
    "    2: 0.2,\n",
    "    3: 0.05,\n",
    "    4: 0.05,\n",
    "    5: 0.15,\n",
    "    6: 0.05,\n",
    "    7: 0.15\n",
    "}\n",
    "\n",
    "weighted_random_sampling(inputs, targets, weighted_indices, m=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "class DataLoader():\n",
    "    \n",
    "    def __init__(self, root='./data'):\n",
    "        self.root = root        \n",
    "        self.trainset = None\n",
    "        self.testset = None\n",
    "        \n",
    "    def download_cifar(self):\n",
    "        print('==> Preparing data..')\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "        # data needs to be loaded through dataloader to get into the correct format\n",
    "        trainset = torchvision.datasets.CIFAR10(root=self.root, train=True, download=True, transform=transform_train)\n",
    "        trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=False, num_workers=2)\n",
    "        self.X_train, self.Y_train = [], []\n",
    "        for x, y in trainloader:\n",
    "            self.X_train.extend(x.numpy()) #numpy needed for a casting workaround\n",
    "            self.Y_train.extend(y.numpy())\n",
    "        self.X_train = np.array(self.X_train)\n",
    "        self.Y_train = np.array(self.Y_train)\n",
    "        \n",
    "        testset = torchvision.datasets.CIFAR10(root=self.root, train=False, download=True, transform=transform_test)\n",
    "        testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n",
    "        self.X_test, self.Y_test = [], []\n",
    "        for x, y in testloader:\n",
    "            self.X_test.extend(x.numpy())\n",
    "            self.Y_test.extend(y.numpy())\n",
    "        self.X_test = np.array(self.X_test)\n",
    "        self.Y_test = np.array(self.Y_test)\n",
    "        \n",
    "        \n",
    "        self.X_batches_train, self.Y_batches_train = None, None\n",
    "        self.X_batches_test, self.Y_batches_test = None, None\n",
    "        \n",
    "    def prepare_cifar(self, strategy, batch_size=128, random_state=None):\n",
    "        self.batch_size = batch_size\n",
    "        assert strategy in ['freeze', 'shuffle', 'homogeneous', 'heterogeneous'], 'Unknown action'\n",
    "        if strategy == 'freeze':\n",
    "            self.X_batches_train, self.Y_batches_train = self.X_train, self.Y_train\n",
    "            self.X_batches_test, self.Y_batches_test = self.X_test, self.Y_test\n",
    "        elif strategy == 'shuffle':\n",
    "            self.X_batches_train, self.Y_batches_train = shuffle(self.X_train, self.Y_train, random_state=random_state)\n",
    "            self.X_batches_test, self.Y_batches_test = shuffle(self.X_test, self.Y_test, random_state=random_state)\n",
    "        elif strategy == 'homogeneous':\n",
    "            self.X_batches_train, self.Y_batches_train = sort_one_class(self.X_train, self.Y_train, self.batch_size, \\\n",
    "                                                            use_shuffle=True, random_state=random_state)\n",
    "            self.X_batches_test, self.Y_batches_test = sort_one_class(self.X_test, self.Y_test, self.batch_size, \\\n",
    "                                                            use_shuffle=True, random_state=random_state)\n",
    "        elif strategy == 'heterogeneous':\n",
    "            self.X_batches_train, self.Y_batches_train = sort_all_classes(self.X_train, self.Y_train, self.batch_size, \\\n",
    "                                                              use_shuffle=True, random_state=random_state)\n",
    "            self.X_batches_test, self.Y_batches_test = sort_all_classes(self.X_test, self.Y_test, self.batch_size, \\\n",
    "                                                              use_shuffle=True, random_state=random_state)\n",
    "    \n",
    "    def yield_batches(self, use_train=True):\n",
    "        batch_idx = 0\n",
    "        X = self.X_batches_train if use_train else self.X_batches_test\n",
    "        Y = self.Y_batches_train if use_train else self.Y_batches_test\n",
    "        X, Y = torch.from_numpy(X), torch.from_numpy(Y)\n",
    "        while batch_idx < len(X):\n",
    "            yield X[batch_idx: batch_idx+self.batch_size], Y[batch_idx: batch_idx+self.batch_size]\n",
    "            batch_idx += self.batch_size\n",
    "            \n",
    "        \n",
    "dataloader = DataLoader()\n",
    "dataloader.download_cifar()\n",
    "dataloader.prepare_cifar('shuffle', random_state=42)\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(dataloader.yield_batches(use_train=True)):\n",
    "    _inputs = inputs\n",
    "    _targets = targets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
